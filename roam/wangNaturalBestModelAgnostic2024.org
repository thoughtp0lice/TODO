:PROPERTIES:
:ID:       42b4a0a3-1bb3-453d-9099-d034bd4ac872
:ROAM_REFS: @wangNaturalBestModelAgnostic2024
:END:
#+title: Natural Is The Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models
#+filetags: :LLM:Code-LLM

* Motivation
- prior methods uses attention information to decide which code tokens should be dropped in code simplification [[id:374d0242-6d95-4b4f-adaa-1f7211b39f59][LLM]]
- author argues that it should only depend on the nature of the code
- findings show that token types is highly impactful
  - there are more symbols (explained in Token Category and Removal) than variables & method signatures in the program
  - but removing symbols results in way lower performance drop

* Method
- Lexical Level
  - Symbol tokens :: defined as the ones including brackets, separators and operators
  - Identifiers :: variable names, method names, etc
  - Syntatic Level :: use AST to identify
    - Tokens in Control Structures :: keywords for control-flow statements, like ~if~, ~for~
    - Tokens in Method Signatures :: tokens in method declaration and parameters
    - Tokens in Method Invocations :: call to methods which include name, parameter return value
- Semantic Level :: keep only  tokens in Program Dependence Graph (PDG)


* Takwaway
- Minimizing the volume of code entered can significantly cut down on training
time.
- Making up a substantial portion of the content yet have minimal impact on
performance, symbols are well-suited for code removal.
- The tokens within method signatures wield the most substantial influence over
downstream tasks, particularly those pertaining to generation tasks.
- Tokens that are not present in PDGs have little impact on downstream tasks due
to their little semantic information
- method signature > identifiers > control structures â‰ˆ method invocations > symbol tokens
